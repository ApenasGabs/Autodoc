# GitHub API
GITHUB_TOKEN=seu_token_github_aqui

# Repositório (opcional, pode ser passado como argumento)
REPO_OWNER=typescript-eslint
REPO_NAME=typescript-eslint
REPO_BRANCH=main

# Configuração LLM
LLM_TYPE=local  # local, api, ou hybrid
LLM_API_KEY=sua_api_key_aqui
LLM_API_URL=https://api.openai.com/v1  # ou outra URL da API

# Configuração para Ollama
OLLAMA_MODEL=llama2  # nome do modelo no Ollama (llama2, codellama, mistral, etc.)
OLLAMA_URL=http://localhost:11434  # URL da API Ollama (padrão: http://localhost:11434)

# Configuração para modelos locais carregados diretamente
LOCAL_MODEL_PATH=/caminho/para/seu/modelo  # apenas para modelos em arquivo local

# Configuração de processamento
MAX_FILE_SIZE_KB=500
MAX_CONCURRENT_PROCESSES=3
EXCLUDE_PATTERNS=node_modules,dist,build,.git,test,__tests__,*.test.*,*.spec.*

# Configuração de saída
OUTPUT_FORMAT=markdown  # markdown ou html
OUTPUT_DIR=./output

# Configurações gerais
LOG_LEVEL=INFO  # DEBUG, INFO, WARN, ERROR
CLEAN_TEMP=true  # true para limpar arquivos temporários após processamento